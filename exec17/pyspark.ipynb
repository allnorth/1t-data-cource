{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/30 09:11:56 WARN Utils: Your hostname, MacBook-Pro-Aleksey.local resolves to a loopback address: 127.0.0.1; using 10.122.249.45 instead (on interface en0)\n",
      "23/08/30 09:11:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/30 09:11:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as w\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local\") \\\n",
    "      .appName(\"Spark\") \\\n",
    "      .config(\"spark.executor.memory\", \"10g\")\\\n",
    "      .config(\"spark.executor.cores\", 5) \\\n",
    "      .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "      .config(\"spark.dynamicAllocation.maxExecutors\", 5) \\\n",
    "      .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 4.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|avg|\n",
      "+---+---+\n",
      "|  1| 71|\n",
      "|  2| 50|\n",
      "+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [(1, 1562007679), (1, 1562007710), (1, 1562007720), (1, 1562007750), (2, 1564682430), (2, 1564682450), (2, 1564682480)]\n",
    "cols = ['id', 'timestamp']\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "\n",
    "window = w.partitionBy('id')\n",
    "\n",
    "df = df.withColumn('min', f.min('timestamp').over(window))\\\n",
    "       .withColumn('max', f.max('timestamp').over(window))\n",
    "\n",
    "df = df.withColumn('avg', f.col('max') - f.col('min'))\n",
    "df.select('id', 'avg').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 4.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 108:======================================>              (145 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+-----+-------------------+\n",
      "|product|location|  week_name|sales|              stock|\n",
      "+-------+--------+-----------+-----+-------------------+\n",
      "|      3|      02|28.08—31.08|  320|                1.0|\n",
      "|      1|      01|01.08—06.08|    1| -749.8333333333334|\n",
      "|      1|      02|21.08—27.08|  770|             -290.0|\n",
      "|      3|      01|14.08—20.08|  490|                1.0|\n",
      "|      1|      02|14.08—20.08|  770|             -290.0|\n",
      "|      2|      01|28.08—31.08|  480|             -180.0|\n",
      "|      3|      01|07.08—13.08|  490|                1.0|\n",
      "|      3|      02|01.08—06.08|  401|                1.0|\n",
      "|      1|      02|07.08—13.08|  770|             -290.0|\n",
      "|      2|      02|28.08—31.08|  360|             -160.0|\n",
      "|      2|      02|01.08—06.08|  201|-133.16666666666666|\n",
      "|      2|      02|07.08—13.08|  630|             -160.0|\n",
      "|      1|      01|21.08—27.08|  700|             -900.0|\n",
      "|      3|      02|07.08—13.08|  560|                1.0|\n",
      "|      2|      01|21.08—27.08|  840|             -180.0|\n",
      "|      2|      02|21.08—27.08|  630|             -160.0|\n",
      "|      1|      02|01.08—06.08|  151|             -241.5|\n",
      "|      1|      01|14.08—20.08|  700|             -900.0|\n",
      "|      3|      02|21.08—27.08|  560|                1.0|\n",
      "|      3|      01|28.08—31.08|  280|                1.0|\n",
      "+-------+--------+-----------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "start_date = dt.datetime(2023, 8, 1)\n",
    "end_date = dt.datetime(2023, 8, 31)\n",
    "\n",
    "dates = pd.date_range(min(start_date, end_date), max(start_date, end_date))\\\n",
    "          .strftime('%Y-%m-%d').tolist()\n",
    "week_num = [dt.datetime.strptime(i, '%Y-%m-%d').date().isocalendar()[1] for i in dates]\n",
    "week = [i - min(week_num) + 1 for i in week_num]\n",
    "data = tuple(zip(dates, week))\n",
    "df_day = spark.createDataFrame(data = data, schema = ['day', 'week'])\n",
    "\n",
    "week_data = (('1', '01.08—06.08'),\n",
    "             ('2', '07.08—13.08'),\n",
    "             ('3', '14.08—20.08'),\n",
    "             ('4', '21.08—27.08'),\n",
    "             ('5', '28.08—31.08'))\n",
    "df_week = spark.createDataFrame(data = week_data, schema = ['week', 'week_name'])\n",
    "\n",
    "demand_data = (('1', '01', 100),\n",
    "               ('1', '02', 110),\n",
    "               ('2', '01', 120),\n",
    "               ('2', '02', 90),\n",
    "               ('3', '01', 70),\n",
    "               ('3', '02', 80))\n",
    "df_demand = spark.createDataFrame(data = demand_data, schema = ['product', 'location', 'demand'])\n",
    "\n",
    "stock_data = (('1', '01', 1000),\n",
    "              ('1', '02', 400),\n",
    "              ('2', '01', 300),\n",
    "              ('2', '02', 250))\n",
    "df_stock = spark.createDataFrame(data = stock_data, schema = ['product', 'location', 'stock'])\n",
    "\n",
    "df = df_demand.join(df_stock, ['product', 'location'], 'left')\\\n",
    "              .select(df_demand['*'], f.coalesce(df_stock.stock, f.lit(0)).alias('stock'))\\\n",
    "              .crossJoin(df_day)\n",
    "\n",
    "wind = w.partitionBy('product', 'location').orderBy('day')\n",
    "\n",
    "df = df.withColumn('sum_demand', f.sum('demand').over(wind))\n",
    "df = df.withColumn('diff', f.col('sum_demand') - f.col('stock'))\n",
    "df = df.withColumn('lag', f.lag('diff', 1, 1).over(wind))\n",
    "df = df.withColumn('sales', when(f.col('lag') > f.col('demand'), f.col('demand'))\\\n",
    "                         .when((f.col('lag') > 0) & (f.col('lag') <= f.col('demand')), f.col('lag'))\n",
    "                         .otherwise(f.lit(0)))\n",
    "df = df.withColumn('min_lag', f.min('lag').over(wind))\n",
    "df = df.join(df_week, ['week'])\n",
    "df = df.groupBy('product', 'location', 'week_name').agg(f.sum('sales').alias('sales'), f.avg('min_lag').alias('stock'))\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
